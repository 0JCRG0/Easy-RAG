{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.prompts import *\n",
    "from voyageai import Client\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "VOYAGE_API_KEY = os.environ.get(\"VOYAGE_API_KEY\")\n",
    "\n",
    "log_format = '%(asctime)s %(levelname)s: %(message)s'\n",
    "\n",
    "logging.basicConfig(filename=\"logging.log\",\n",
    "\tlevel=logging.INFO,\n",
    "\tformat=log_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdToBatches(data: str, max_tokens: int = 512, print_messages: bool = False) -> list:\n",
    "\tbatches = []\n",
    "\ttotal_tokens = 0\n",
    "\ttruncation_counter = 0  # Counter for truncations\n",
    "\n",
    "\t# Split the data into sections based on H1 headings\n",
    "\tsections = re.split(r\"(?m)^#\\s+\", data)[1:]\n",
    "\n",
    "\tfor section in sections:\n",
    "\t\t# Extract the H1 heading\n",
    "\t\th1_match = re.match(r\"^(.*?)$\", section, re.MULTILINE)\n",
    "\t\th1 = h1_match.group(1).strip() if h1_match else \"\"\n",
    "\n",
    "\t\t# Split the section into subsections based on H2 headings\n",
    "\t\tsubsections = re.split(r\"(?m)^##\\s+\", section)[1:]\n",
    "\n",
    "\t\tfor subsection in subsections:\n",
    "\t\t\t# Extract the H2 heading\n",
    "\t\t\th2_match = re.match(r\"^(.*?)$\", subsection, re.MULTILINE)\n",
    "\t\t\th2 = h2_match.group(1).strip() if h2_match else \"\"\n",
    "\n",
    "\t\t\t# Extract the text content\n",
    "\t\t\ttext = re.sub(r\"^(#|##).*$\", \"\", subsection, flags=re.MULTILINE).strip()\n",
    "\n",
    "\t\t\t# Format the entry as (H1)[H2] \"text\"\n",
    "\t\t\tentry = f\"({h1})[{h2}] \\\"{text}\\\"\"\n",
    "\n",
    "\t\t\ttokens_description = num_tokens(entry)\n",
    "\t\t\tif tokens_description <= max_tokens:\n",
    "\t\t\t\tbatches.append(entry)\n",
    "\t\t\t\ttotal_tokens += tokens_description\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Truncate and create new batches with the remaining text\n",
    "\t\t\t\tremaining_text = text\n",
    "\t\t\t\twhile len(remaining_text) > 0:\n",
    "\t\t\t\t\ttruncated_text = truncated_string(remaining_text, model=\"gpt-3.5-turbo\", max_tokens=max_tokens)\n",
    "\t\t\t\t\ttruncated_entry = f\"({h1})[{h2}] \\\"{truncated_text}\\\"\"\n",
    "\t\t\t\t\tbatches.append(truncated_entry)\n",
    "\t\t\t\t\ttotal_tokens += num_tokens(truncated_entry)\n",
    "\t\t\t\t\ttruncation_counter += 1\n",
    "\t\t\t\t\tremaining_text = remaining_text[len(truncated_text):]\n",
    "\n",
    "\tapproximate_cost = 0 #TODO: Update\n",
    "\taverage_tokens_per_batch = total_tokens / len(batches)\n",
    "\t\n",
    "\tlog_data = {\n",
    "\t\t\"TOTAL NUMBER OF BATCHES\": len(batches),\n",
    "\t\t\"TOTAL NUMBER OF TOKENS\": total_tokens,\n",
    "\t\t\"MAX TOKENS PER BATCH\": max_tokens,\n",
    "\t\t\"NUMBER OF TRUNCATIONS\": truncation_counter,\n",
    "\t\t\"AVERAGE NUMBER OF TOKENS PER BATCH\": round(average_tokens_per_batch, 2),\n",
    "\t\t\"APPROXIMATE COST OF EMBEDDING\": f\"${round(approximate_cost, 2)} USD\"\n",
    "\t}\n",
    "\t\n",
    "\tlogging.info(json.dumps(log_data))\n",
    "\n",
    "\tif print_messages:\n",
    "\t\tfor i, batch in enumerate(batches, start=1):\n",
    "\t\t\tprint(f\"Batch {i}:\")\n",
    "\t\t\tprint(batch)\n",
    "\t\t\tprint(f\"Tokens per batch:\", num_tokens(batch))\n",
    "\t\t\tprint(\"\\n\")\n",
    "\t\tprint(log_data)\n",
    "\n",
    "\treturn batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPDF(pdf_file_path: str) -> str:\n",
    "\tloader = PyMuPDFLoader(pdf_file_path)\n",
    "\n",
    "\tpdf_data = loader.load()\n",
    "\n",
    "\tpdf_data\n",
    "\n",
    "\tdata = []\n",
    "\n",
    "\tdef clean_pdf(content):\n",
    "\t\tcontent = re.sub(r'\\s+', ' ', content)\n",
    "\t\tlines = [line.strip() for line in content.splitlines() if line.strip()]\n",
    "\t\tcleaned_content = '\\n'.join(lines)\n",
    "\t\treturn cleaned_content\n",
    "\n",
    "\tfor page in pdf_data:\n",
    "\t\t_text = page.page_content\n",
    "\t\ttext = clean_pdf(_text)\n",
    "\t\tprint(text)\n",
    "\n",
    "\t\tdata.append(text)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdfToBatches(data: list, max_tokens: int = 512, print_messages: bool = False) -> list:\n",
    "\tbatches = []\n",
    "\ttotal_tokens = 0\n",
    "\ttruncation_counter = 0  # Counter for truncations\n",
    "\n",
    "\tfor entry in data:\n",
    "\t\t#text = \" \".join(i)  # Join the elements of the list into a single string\n",
    "\t\ttokens_description = num_tokens(entry)\n",
    "\t\tif tokens_description <= max_tokens:\n",
    "\t\t\tbatches.append(entry)\n",
    "\t\telse:\n",
    "\t\t\t#TRUNCATE IF STRING MORE THAN x TOKENS\n",
    "\t\t\tjob_truncated = truncated_string(entry, model=\"gpt-3.5-turbo\", max_tokens=max_tokens)\n",
    "\t\t\tbatches.append(job_truncated)\n",
    "\t\t\ttruncation_counter += 1\n",
    "\n",
    "\t\ttotal_tokens += num_tokens(entry)\n",
    "\n",
    "\tapproximate_cost = 0\n",
    "\n",
    "\taverage_tokens_per_batch = total_tokens / len(batches)\n",
    "\t\n",
    "\tlog_data = {\n",
    "\t\t\"TOTAL NUMBER OF BATCHES\": len(batches),\n",
    "\t\t\"TOTAL NUMBER OF TOKENS\": total_tokens,\n",
    "\t\t\"MAX TOKENS PER BATCH\": max_tokens,\n",
    "\t\t\"NUMBER OF TRUNCATIONS\": truncation_counter,\n",
    "\t\t\"AVERAGE NUMBER OF TOKENS PER BATCH\": round(average_tokens_per_batch, 2),\n",
    "\t\t\"APPROXIMATE COST OF EMBEDDING\": f\"${round(approximate_cost, 2)} USD\"\n",
    "\t}\n",
    "\t\n",
    "\tlogging.info(json.dumps(log_data))\n",
    "\n",
    "\tif print_messages:\n",
    "\t\tfor i, batch in enumerate(batches, start=1):\n",
    "\t\t\tprint(f\"Batch {i}:\")\n",
    "\t\t\tprint(\"\".join(batch))\n",
    "\t\t\tprint(f\"Tokens per batch:\", num_tokens(batch))\n",
    "\t\t\tprint(\"\\n\")\n",
    "\n",
    "\t\tprint(log_data)\n",
    "\t\n",
    "\treturn batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call either md or pdf\n",
    "batches = mdToBatches(apricot_moose_md, 512)\n",
    "#batches = pdfToBatches(data, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vo = Client(api_key=VOYAGE_API_KEY)\n",
    "\n",
    "result = vo.embed(batches, model=\"voyage-2\", input_type=\"document\", truncation=False)\n",
    "\n",
    "pd_data = {\n",
    "\t\"chunks\": batches,\n",
    "\t\"embeddings\": result.embeddings\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(pd_data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_postgre(df, table=\"apricot_moose\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
