{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\tformat=log_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdToBatches(data: str, max_tokens: int = 512, print_messages: bool = False) -> list:\n",
    "\tbatches = []\n",
    "\ttotal_tokens = 0\n",
    "\ttruncation_counter = 0  # Counter for truncations\n",
    "\n",
    "\t# Split the data into sections based on H1 headings\n",
    "\tsections = re.split(r\"(?m)^#\\s+\", data)[1:]\n",
    "\n",
    "\tfor section in sections:\n",
    "\t\t# Extract the H1 heading\n",
    "\t\th1_match = re.match(r\"^(.*?)$\", section, re.MULTILINE)\n",
    "\t\th1 = h1_match.group(1).strip() if h1_match else \"\"\n",
    "\n",
    "\t\t# Split the section into subsections based on H2 headings\n",
    "\t\tsubsections = re.split(r\"(?m)^##\\s+\", section)[1:]\n",
    "\n",
    "\t\tfor subsection in subsections:\n",
    "\t\t\t# Extract the H2 heading\n",
    "\t\t\th2_match = re.match(r\"^(.*?)$\", subsection, re.MULTILINE)\n",
    "\t\t\th2 = h2_match.group(1).strip() if h2_match else \"\"\n",
    "\n",
    "\t\t\t# Extract the text content\n",
    "\t\t\ttext = re.sub(r\"^(#|##).*$\", \"\", subsection, flags=re.MULTILINE).strip()\n",
    "\n",
    "\t\t\t# Format the entry as (H1)[H2] \"text\"\n",
    "\t\t\tentry = f\"({h1})[{h2}] \\\"{text}\\\"\"\n",
    "\n",
    "\t\t\ttokens_description = num_tokens(entry)\n",
    "\t\t\tif tokens_description <= max_tokens:\n",
    "\t\t\t\tbatches.append(entry)\n",
    "\t\t\t\ttotal_tokens += tokens_description\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Truncate and create new batches with the remaining text\n",
    "\t\t\t\tremaining_text = text\n",
    "\t\t\t\twhile len(remaining_text) > 0:\n",
    "\t\t\t\t\ttruncated_text = truncated_string(remaining_text, model=\"gpt-3.5-turbo\", max_tokens=max_tokens)\n",
    "\t\t\t\t\ttruncated_entry = f\"({h1})[{h2}] \\\"{truncated_text}\\\"\"\n",
    "\t\t\t\t\tbatches.append(truncated_entry)\n",
    "\t\t\t\t\ttotal_tokens += num_tokens(truncated_entry)\n",
    "\t\t\t\t\ttruncation_counter += 1\n",
    "\t\t\t\t\tremaining_text = remaining_text[len(truncated_text):]\n",
    "\n",
    "\tapproximate_cost = 0 #TODO: Update\n",
    "\taverage_tokens_per_batch = total_tokens / len(batches)\n",
    "\t\n",
    "\tlog_data = {\n",
    "\t\t\"TOTAL NUMBER OF BATCHES\": len(batches),\n",
    "\t\t\"TOTAL NUMBER OF TOKENS\": total_tokens,\n",
    "\t\t\"MAX TOKENS PER BATCH\": max_tokens,\n",
    "\t\t\"NUMBER OF TRUNCATIONS\": truncation_counter,\n",
    "\t\t\"AVERAGE NUMBER OF TOKENS PER BATCH\": round(average_tokens_per_batch, 2),\n",
    "\t\t\"APPROXIMATE COST OF EMBEDDING\": f\"${round(approximate_cost, 2)} USD\"\n",
    "\t}\n",
    "\t\n",
    "\tlogging.info(json.dumps(log_data))\n",
    "\n",
    "\tif print_messages:\n",
    "\t\tfor i, batch in enumerate(batches, start=1):\n",
    "\t\t\tprint(f\"Batch {i}:\")\n",
    "\t\t\tprint(batch)\n",
    "\t\t\tprint(f\"Tokens per batch:\", num_tokens(batch))\n",
    "\t\t\tprint(\"\\n\")\n",
    "\t\tprint(log_data)\n",
    "\n",
    "\treturn batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPDF(pdf_file_path: str) -> str:\n",
    "\tloader = PyMuPDFLoader(pdf_file_path)\n",
    "\n",
    "\tpdf_data = loader.load()\n",
    "\n",
    "\tpdf_data\n",
    "\n",
    "\tdata = []\n",
    "\n",
    "\tdef clean_pdf(content):\n",
    "\t\tcontent = re.sub(r'\\s+', ' ', content)\n",
    "\t\tlines = [line.strip() for line in content.splitlines() if line.strip()]\n",
    "\t\tcleaned_content = '\\n'.join(lines)\n",
    "\t\treturn cleaned_content\n",
    "\n",
    "\tfor page in pdf_data:\n",
    "\t\t_text = page.page_content\n",
    "\t\ttext = clean_pdf(_text)\n",
    "\t\tprint(text)\n",
    "\n",
    "\t\tdata.append(text)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdfToBatches(data: list, max_tokens: int = 512, print_messages: bool = True) -> list:\n",
    "\tbatches = []\n",
    "\ttotal_tokens = 0\n",
    "\ttruncation_counter = 0  # Counter for truncations\n",
    "\n",
    "\tfor entry in data:\n",
    "\t\t#text = \" \".join(i)  # Join the elements of the list into a single string\n",
    "\t\ttokens_description = num_tokens(entry)\n",
    "\t\tif tokens_description <= max_tokens:\n",
    "\t\t\tbatches.append(entry)\n",
    "\t\telse:\n",
    "\t\t\t#TRUNCATE IF STRING MORE THAN x TOKENS\n",
    "\t\t\tjob_truncated = truncated_string(entry, model=\"gpt-3.5-turbo\", max_tokens=max_tokens)\n",
    "\t\t\tbatches.append(job_truncated)\n",
    "\t\t\ttruncation_counter += 1\n",
    "\n",
    "\t\ttotal_tokens += num_tokens(entry)\n",
    "\n",
    "\tapproximate_cost = 0\n",
    "\n",
    "\taverage_tokens_per_batch = total_tokens / len(batches)\n",
    "\t\n",
    "\tlog_data = {\n",
    "\t\t\"TOTAL NUMBER OF BATCHES\": len(batches),\n",
    "\t\t\"TOTAL NUMBER OF TOKENS\": total_tokens,\n",
    "\t\t\"MAX TOKENS PER BATCH\": max_tokens,\n",
    "\t\t\"NUMBER OF TRUNCATIONS\": truncation_counter,\n",
    "\t\t\"AVERAGE NUMBER OF TOKENS PER BATCH\": round(average_tokens_per_batch, 2),\n",
    "\t\t\"APPROXIMATE COST OF EMBEDDING\": f\"${round(approximate_cost, 2)} USD\"\n",
    "\t}\n",
    "\t\n",
    "\tlogging.info(json.dumps(log_data))\n",
    "\n",
    "\tif print_messages:\n",
    "\t\tfor i, batch in enumerate(batches, start=1):\n",
    "\t\t\tprint(f\"Batch {i}:\")\n",
    "\t\t\tprint(\"\".join(batch))\n",
    "\t\t\tprint(f\"Tokens per batch:\", num_tokens(batch))\n",
    "\t\t\tprint(\"\\n\")\n",
    "\n",
    "\t\tprint(log_data)\n",
    "\t\n",
    "\treturn batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call either md or pdf\n",
    "batches = mdToBatches(CohereSafetyGuideline, 512)\n",
    "#batches = pdfToBatches(data, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import time\n",
    "\n",
    "for i in range(0, len(batches), 8):\n",
    "    time.sleep(2)\n",
    "    chunk = batches[i:i+10]\n",
    "    clean_chunk = askAnthropic(cleanEntries, chunk)\n",
    "    print(clean_chunk)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                             chunks  \\\n",
      "0    0  (🤖 Introduction to Command)[🪸 CoraltheChatbot]...   \n",
      "1    1  (🎙 WritingEffectivePrompts)[✏ PromptWritingTip...   \n",
      "2    2  (🎙 WritingEffectivePrompts)[✏ PromptWritingTip...   \n",
      "3    3  (✅ IdealModelBehavior)[BehaviorPrinciples] \"Be...   \n",
      "4    4  (✅ IdealModelBehavior)[WritingDefaults] \"Writi...   \n",
      "5    5  (✅ IdealModelBehavior)[WritingDefaults] \"```\\n...   \n",
      "6    6  (✅ IdealModelBehavior)[Capabilities and Limita...   \n",
      "7    7  (✅ IdealModelBehavior)[Capabilities and Limita...   \n",
      "8    8  (✅ IdealModelBehavior)[Capabilities and Limita...   \n",
      "9    9  (✅ IdealModelBehavior)[Self-Referencevs.Self-A...   \n",
      "10  10  (✅ IdealModelBehavior)[Self-Referencevs.Self-A...   \n",
      "11  11  (✅ IdealModelBehavior)[🦺 Safety] \"🦺 Safety\\n\\n...   \n",
      "12  12  (✅ IdealModelBehavior)[🦺 Safety] \"ensual.\\n```...   \n",
      "13  13  (✅ IdealModelBehavior)[🦺 Safety] \"self-anthrop...   \n",
      "14  14  (✅ IdealModelBehavior)[🦺 Safety] \" WhatisMeinK...   \n",
      "15  15  (✅ IdealModelBehavior)[🦺 Safety] \"response,ori...   \n",
      "16  16  (✅ IdealModelBehavior)[Refusals] \"Refusals\\n\\n...   \n",
      "17  17  (✅ IdealModelBehavior)[Refusals] \"```\\n```\\nUs...   \n",
      "18  18  (✅ IdealModelBehavior)[WritingQuality] \"Writin...   \n",
      "19  19  (✅ IdealModelBehavior)[WritingQuality] \"puted\\...   \n",
      "20  20  (✅ IdealModelBehavior)[WritingQuality] \"’sresp...   \n",
      "21  21  (✅ IdealModelBehavior)[WritingQuality] \"should...   \n",
      "22  22  (✅ IdealModelBehavior)[WritingQuality] \"ulatio...   \n",
      "23  23  (✅ IdealModelBehavior)[WritingQuality] \"Lackof...   \n",
      "24  24  (✅ IdealModelBehavior)[WritingQuality] \"eringa...   \n",
      "25  25  (✅ IdealModelBehavior)[WritingQuality] \"alisto...   \n",
      "26  26  (✅ IdealModelBehavior)[WritingQuality] \"ponse....   \n",
      "27  27  (👗Style Guidelines)[ResponseLength] \"ResponseL...   \n",
      "28  28  (👗Style Guidelines)[QuestionAnswering] \"Questi...   \n",
      "29  29  (👗Style Guidelines)[QuestionAnswering] \"confid...   \n",
      "30  30        (👗Style Guidelines)[QuestionAnswering] \"7.\"   \n",
      "31  31  (👗Style Guidelines)[Lists] \"Lists\\n\\nSometimes...   \n",
      "32  32  (👗Style Guidelines)[Lists] \"ofthetreewheretheb...   \n",
      "33  33  (👗Style Guidelines)[Essays,Blogs,andLongformRe...   \n",
      "34  34  (👗Style Guidelines)[Summarization] \"Summarizat...   \n",
      "35  35  (👗Style Guidelines)[Extraction] \"Extraction\\n\\...   \n",
      "36  36  (👗Style Guidelines)[Markdown] \"Markdown\\n\\nThe...   \n",
      "37  37  (👗Style Guidelines)[Math] \"Math\\n\\nInPANDA+(En...   \n",
      "38  38  (👗Style Guidelines)[Math] \")=f^{-1}(x)=-\frac{1...   \n",
      "39  39  (👗Style Guidelines)[LaTeX] \"LaTeX\\n\\nThemodels...   \n",
      "\n",
      "                                           embeddings  \n",
      "0   [0.016479158774018288, 0.02702033706009388, 0....  \n",
      "1   [0.004363069776445627, 0.02634543552994728, 0....  \n",
      "2   [-0.007568052038550377, 0.042701661586761475, ...  \n",
      "3   [-0.025345075875520706, 0.03494488447904587, 0...  \n",
      "4   [-0.020084882155060768, 0.034845512360334396, ...  \n",
      "5   [-0.025834640488028526, 0.01824279874563217, 0...  \n",
      "6   [-0.025799404829740524, 0.03539338707923889, 0...  \n",
      "7   [-0.0177596565335989, 0.037614889442920685, 0....  \n",
      "8   [-0.04131174087524414, 0.041885487735271454, 0...  \n",
      "9   [-0.0005486651207320392, 0.02967739850282669, ...  \n",
      "10  [-0.032889626920223236, 0.047765202820301056, ...  \n",
      "11  [-0.01868237741291523, 0.04688122496008873, 0....  \n",
      "12  [-0.04462296888232231, 0.04226117208600044, 0....  \n",
      "13  [-0.034216828644275665, 0.018131496384739876, ...  \n",
      "14  [-0.008323314599692822, 0.02831651084125042, -...  \n",
      "15  [-0.03086102195084095, 0.03301703929901123, 0....  \n",
      "16  [-0.015541587956249714, 0.03275024890899658, 0...  \n",
      "17  [-0.02186441235244274, 0.026334425434470177, 0...  \n",
      "18  [-0.03429168462753296, 0.03165031224489212, -0...  \n",
      "19  [-0.037021197378635406, 0.06645084172487259, 0...  \n",
      "20  [-0.03556053340435028, 0.024958917871117592, 0...  \n",
      "21  [-0.013833248056471348, 0.013863885775208473, ...  \n",
      "22  [-0.03532152250409126, 0.03093508817255497, -0...  \n",
      "23  [-0.012342363595962524, 0.04146828129887581, 0...  \n",
      "24  [0.006880635395646095, 0.0239410400390625, 0.0...  \n",
      "25  [-0.02870848961174488, 0.03838932514190674, 0....  \n",
      "26  [0.00021982919133733958, 0.018401989713311195,...  \n",
      "27  [-0.052701178938150406, 0.028381818905472755, ...  \n",
      "28  [-0.01817384548485279, 0.009621917270123959, 0...  \n",
      "29  [-0.014546811580657959, -1.2210423847136553e-0...  \n",
      "30  [0.0012565446086227894, 0.04672924801707268, 0...  \n",
      "31  [-0.03691855072975159, 0.018296590074896812, -...  \n",
      "32  [-0.031129932031035423, 0.021867133677005768, ...  \n",
      "33  [0.010814141482114792, 0.023446206003427505, 0...  \n",
      "34  [-0.0228121355175972, 0.03419249504804611, 0.0...  \n",
      "35  [-0.037772778421640396, 0.01377807930111885, 0...  \n",
      "36  [-0.029833614826202393, 0.020877474918961525, ...  \n",
      "37  [-0.002474728040397167, 0.026529472321271896, ...  \n",
      "38  [-0.030764760449528694, 0.031041964888572693, ...  \n",
      "39  [-0.02084321342408657, 0.035849038511514664, -...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vo = Client(api_key=VOYAGE_API_KEY)\n",
    "\n",
    "result = vo.embed(batches, model=\"voyage-2\", input_type=\"document\", truncation=False)\n",
    "\n",
    "pd_data = {\n",
    "    \"id\": list(range(len(batches))), \n",
    "\t\"chunks\": batches,\n",
    "\t\"embeddings\": result.embeddings\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(pd_data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_postgre(df, table=\"apricot_moose\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
